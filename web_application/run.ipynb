{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/mydrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_M0lNRlVJSn",
        "outputId": "633a9d30-51de-4450-b1b0-4412a757e4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/mydrive/; to attempt to forcibly remount, call drive.mount(\"/content/mydrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4X1Y9U9FaJm",
        "outputId": "7847fed6-e89e-4100-afee-911a6110f391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.8/dist-packages (5.2.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from pyngrok) (6.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.8/dist-packages (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (4.4.0)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.8/dist-packages (from streamlit) (0.8.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.8/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from streamlit) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.8/dist-packages (from streamlit) (5.1.0)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.8/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (12.6.0)\n",
            "Requirement already satisfied: validators>=0.2 in /usr/local/lib/python3.8/dist-packages (from streamlit) (0.20.0)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.8/dist-packages (from streamlit) (2.13.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19 in /usr/local/lib/python3.8/dist-packages (from streamlit) (3.1.29)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (6.0.4)\n",
            "Requirement already satisfied: requests>=2.4 in /usr/local/lib/python3.8/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (4.2.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (5.2.0)\n",
            "Requirement already satisfied: pympler>=0.9 in /usr/local/lib/python3.8/dist-packages (from streamlit) (1.0.1)\n",
            "Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.8/dist-packages (from streamlit) (21.3)\n",
            "Requirement already satisfied: blinker>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (1.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.8/dist-packages (from streamlit) (2.2.0)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit) (0.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit) (4.3.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from gitpython!=3.1.19->streamlit) (4.0.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=1.4->streamlit) (3.11.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=14.1->streamlit) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.21.0->streamlit) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil->streamlit) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4->streamlit) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4->streamlit) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4->streamlit) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4->streamlit) (2.10)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.11.0->streamlit) (2.6.1)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.11.0->streamlit) (0.9.1)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from validators>=0.2->streamlit) (4.4.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit) (5.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.19.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (5.10.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (22.1.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf<=3.20.1\n",
            "  Using cached protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "onnx 1.13.0 requires protobuf<4,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "googleapis-common-protos 1.57.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-translate 3.8.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-language 2.6.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-firestore 2.7.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-datastore 2.9.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery 3.3.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.16.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.8/dist-packages (1.13.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.12)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.21.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (21.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->onnxruntime) (3.0.9)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->onnxruntime) (1.2.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mAuthtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok\n",
        "!pip install streamlit -q\n",
        "!pip install streamlit\n",
        "!pip install --upgrade \"protobuf<=3.20.1\"\n",
        "!pip3 install onnx>=1.10.0\n",
        "!pip install onnxruntime\n",
        "!pip install Image\n",
        "!ngrok authtoken 2J1ZUMSnlmVliyGybpSQnGHacdu_YcDNF3R3DaNfFvrEH6pA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st"
      ],
      "metadata": {
        "id": "9y6fMDnDa_yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/mydrive/Othercomputers/MacPro/12sw_project/web_application"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15A_hRHjBz4b",
        "outputId": "d0047086-3e88-4aa1-837d-d842bc051879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mydrive/Othercomputers/MacPro/12sw_project/web_application\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 내용확인\n",
        "!cat app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1H85q8FRSpA",
        "outputId": "a8ed5110-08fb-4ae0-f9d0-dafb79437572"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import streamlit as st\n",
            "import numpy as np\n",
            "from detection.object_detection import detect_object, detect_dic\n",
            "from detection.image_save import load_image\n",
            "from siames.siames_network import preprocess_image, generate_embedding\n",
            "import cv2\n",
            "import os\n",
            "from yolov7 import YOLOv7\n",
            "from PIL import Image, ImageOps\n",
            "import pandas as pd\n",
            "from tensorflow.keras.models import load_model\n",
            "from tqdm import tqdm\n",
            "from skimage import io\n",
            "from skimage.transform import resize\n",
            "\n",
            "st.set_page_config(page_title=\"AI Style Recommender\")\n",
            "\n",
            "st.image('logo.png')\n",
            "\n",
            "st.sidebar.title(\"AI Style Recommender\")\n",
            "st.sidebar.caption(\"Buy the style you want.\")\n",
            "st.sidebar.markdown(\"Made by Style Research Team\")\n",
            "st.sidebar.markdown(\"---\")\n",
            "\n",
            "# 여기서는 column의 형식으로 \n",
            "\n",
            "st.sidebar.header(\"Our DB\")\n",
            "st.sidebar.image(\"https://media.giphy.com/media/l8DnhngGazFTPhNLK6/giphy.gif\",width=300)\n",
            "st.sidebar.markdown('---')\n",
            "\n",
            "\n",
            "st.write('# Detect Your Style')\n",
            "\n",
            "uploaded_image = st.file_uploader(\"이미지를 넣어주세요.\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
            "\n",
            "if uploaded_image is not None:\n",
            "    col1,col2 = st.columns(2)\n",
            "    file_bytes = np.asarray(bytearray(uploaded_image.read()), dtype=np.uint8)\n",
            "    opencv_image = cv2.imdecode(file_bytes, 1)\n",
            "    with col1:\n",
            "      st.image(opencv_image,caption='입력한 이미지' ,channels=\"BGR\")\n",
            "    cv2.imwrite('image/query.jpg',opencv_image)\n",
            "    image = detect_object(opencv_image)\n",
            "    cv2.imwrite(\"image/Detected_objects.jpg\",image)\n",
            "    with col2:\n",
            "      st.image('image/Detected_objects.jpg',caption='이미지 인식 결과')\n",
            "\n",
            "\n",
            "    st.markdown('## Detection 결과')\n",
            "    try: \n",
            "      detect = detect_dic(opencv_image)\n",
            "      \n",
            "      df1 = pd.DataFrame(detect) \n",
            "  \n",
            "      # st.dataframe(df1, 800, 150)\n",
            "\n",
            "      # 샴 모델 불러오기 \n",
            "      \n",
            "      if detect['class_id'] is not None:\n",
            "              set_1 = set(detect['class_id'])\n",
            "              set_2 = set(['001001','001005','001011'])\n",
            "              set_3 = set(['002021','002007','002012'])\n",
            "              set_4 = set(['003005','003002','003009'])\n",
            "              set_5 = set(['005004','005011','018002'])\n",
            "              if set_1.isdisjoint(set_2):\n",
            "                  st.markdown('### 상의')\n",
            "                  st.write('상의를 찾지 못하였습니다.')\n",
            "                  st.markdown('---')\n",
            "              else:\n",
            "                for i in set_1.intersection(set_2):\n",
            "                  st.markdown('### 상의')\n",
            "                  Midclass = load_model(\"models/Midcateg_\" + i + \"_1216.h5\", compile=False)\n",
            "                  df = pd.read_csv(\"DB/df_styleReview_\" + i + \"_sim_matrix.csv\", dtype=object)\n",
            "              \n",
            "                  # 관련된 미리 학습된 siamse df을 불러와 준다. \n",
            "                  \n",
            "                  tqdm.pandas()\n",
            "                  xmin = df1.loc[df1['class_id'] == i,'xmin'].values[0]\n",
            "                  ymax = df1.loc[df1['class_id'] == i,'ymax'].values[0]\n",
            "                  xmax = df1.loc[df1['class_id'] == i,'xmax'].values[0]\n",
            "                  ymin = df1.loc[df1['class_id'] == i,'ymin'].values[0]\n",
            "                  img = cv2.imread(\"image/query.jpg\")\n",
            "                  crop_img = img[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
            "                  cv2.imwrite('image/croppedImage' + i + '.PNG',crop_img)\n",
            "                  query_dir = 'image/croppedImage' + i + '.PNG' \n",
            "                  query = query_dir\n",
            "                  # csv 파일을 통해 해주자. \n",
            "                  query_image = preprocess_image(query)\n",
            "                  # 크롭된 이미지를 넣어주어서 불러온다. \n",
            "                  query_embedding_ = Midclass(query_image)[0].numpy().astype(np.float32).tolist()\n",
            "                  query_embedding_ = str(query_embedding_)\n",
            "\n",
            "                  df['distance_samImg_queryRev'] = df['embedding_sample'].progress_apply(lambda x: np.linalg.norm(np.asarray(eval(x), dtype=np.float32) - np.asarray(eval(query_embedding_), dtype=np.float32)))\n",
            "                  df_sim_matrix = df.sort_values(by='distance_samImg_queryRev').reset_index(drop=True)\n",
            "                  # 이밑은 image로 해준다. print은 write로 바꿔준다.\n",
            "                  \n",
            "                  image_ = io.imread(query)\n",
            "                  image_resized = resize(image_, (200, 200))\n",
            "                  st.image(image_resized,caption='Query Image') \n",
            "                      \n",
            "                  col1,col2,col3,col4,col5 = st.columns(5)\n",
            "                  with col1:\n",
            "                    st.write('유사상품1')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[0, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)\n",
            "                  with col2:\n",
            "                    st.write('유사상품2')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[1, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)   \n",
            "                  with col3:\n",
            "                    st.write('유사상품3')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[2, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)   \n",
            "                  with col4:\n",
            "                    st.write('유사상품4')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[3, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)   \n",
            "                  with col5:\n",
            "                    st.write('유사상품5')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[4, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True) \n",
            "                  st.markdown('---')\n",
            "\n",
            "              if set_1.isdisjoint(set_3):\n",
            "                  st.markdown('### 아우터')\n",
            "                  st.write('아우터를 찾지 못하였습니다.')\n",
            "                  st.markdown('---')\n",
            "              else:\n",
            "                for i in set_1.intersection(set_3):\n",
            "                  st.markdown('### 아우터')\n",
            "                  Midclass = load_model(\"models/Midcateg_\" + i + \"_1216.h5\", compile=False)\n",
            "                  df = pd.read_csv(\"DB/df_styleReview_\" + i + \"_sim_matrix.csv\", dtype=object)\n",
            "              \n",
            "                  # 관련된 미리 학습된 siamse df을 불러와 준다. \n",
            "                  \n",
            "                  tqdm.pandas()\n",
            "                  xmin = df1.loc[df1['class_id'] == i,'xmin'].values[0]\n",
            "                  ymax = df1.loc[df1['class_id'] == i,'ymax'].values[0]\n",
            "                  xmax = df1.loc[df1['class_id'] == i,'xmax'].values[0]\n",
            "                  ymin = df1.loc[df1['class_id'] == i,'ymin'].values[0]\n",
            "                  img = cv2.imread(\"image/query.jpg\")\n",
            "                  crop_img = img[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
            "                  cv2.imwrite('image/croppedImage' + i + '.PNG',crop_img)\n",
            "                  query_dir = 'image/croppedImage' + i + '.PNG' \n",
            "                  query = query_dir\n",
            "                  # csv 파일을 통해 해주자. \n",
            "                  query_image = preprocess_image(query)\n",
            "                  # 크롭된 이미지를 넣어주어서 불러온다. \n",
            "                  query_embedding_ = Midclass(query_image)[0].numpy().astype(np.float32).tolist()\n",
            "                  query_embedding_ = str(query_embedding_)\n",
            "\n",
            "                  df['distance_samImg_queryRev'] = df['embedding_sample'].progress_apply(lambda x: np.linalg.norm(np.asarray(eval(x), dtype=np.float32) - np.asarray(eval(query_embedding_), dtype=np.float32)))\n",
            "                  df_sim_matrix = df.sort_values(by='distance_samImg_queryRev').reset_index(drop=True)\n",
            "                  # 이밑은 image로 해준다. print은 write로 바꿔준다.\n",
            "                  \n",
            "                  image_ = io.imread(query)\n",
            "                  image_resized = resize(image_, (200, 200))\n",
            "                  st.image(image_resized,caption='Query Image') \n",
            "                      \n",
            "                  col1,col2,col3,col4,col5 = st.columns(5)\n",
            "                  with col1:\n",
            "                    st.write('유사상품1')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[0, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)\n",
            "                  with col2:\n",
            "                    st.write('유사상품2')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[1, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)   \n",
            "                  with col3:\n",
            "                    st.write('유사상품3')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[2, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)   \n",
            "                  with col4:\n",
            "                    st.write('유사상품4')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[3, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)   \n",
            "                  with col5:\n",
            "                    st.write('유사상품5')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[4, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)\n",
            "                  st.markdown('---')  \n",
            "\n",
            "\n",
            "\n",
            "              if set_1.isdisjoint(set_4):\n",
            "                  st.markdown('### 하의')\n",
            "                  st.write('하의를 찾지 못하였습니다.')\n",
            "                  st.markdown('---')\n",
            "              else:\n",
            "                for i in set_1.intersection(set_4):\n",
            "                  st.markdown('### 하의')\n",
            "                  Midclass = load_model(\"models/Midcateg_\" + i + \"_1216.h5\", compile=False)\n",
            "                  df = pd.read_csv(\"DB/df_styleReview_\" + i + \"_sim_matrix.csv\", dtype=object)\n",
            "              \n",
            "                  # 관련된 미리 학습된 siamse df을 불러와 준다. \n",
            "                  \n",
            "                  tqdm.pandas()\n",
            "                  xmin = df1.loc[df1['class_id'] == i,'xmin'].values[0]\n",
            "                  ymax = df1.loc[df1['class_id'] == i,'ymax'].values[0]\n",
            "                  xmax = df1.loc[df1['class_id'] == i,'xmax'].values[0]\n",
            "                  ymin = df1.loc[df1['class_id'] == i,'ymin'].values[0]\n",
            "                  img = cv2.imread(\"image/query.jpg\")\n",
            "                  crop_img = img[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
            "                  cv2.imwrite('image/croppedImage' + i + '.PNG',crop_img)\n",
            "                  query_dir = 'image/croppedImage' + i + '.PNG' \n",
            "                  query = query_dir\n",
            "                  query_image = preprocess_image(query)\n",
            "                  # 크롭된 이미지를 넣어주어서 불러온다. \n",
            "                  query_embedding_ = Midclass(query_image)[0].numpy().astype(np.float32).tolist()\n",
            "                  query_embedding_ = str(query_embedding_)\n",
            "\n",
            "                  df['distance_samImg_queryRev'] = df['embedding_sample'].progress_apply(lambda x: np.linalg.norm(np.asarray(eval(x), dtype=np.float32) - np.asarray(eval(query_embedding_), dtype=np.float32)))\n",
            "                  df_sim_matrix = df.sort_values(by='distance_samImg_queryRev').reset_index(drop=True)\n",
            "                  # 이밑은 image로 해준다. print은 write로 바꿔준다.\n",
            "                  \n",
            "                  image_ = io.imread(query)\n",
            "                  image_resized = resize(image_, (200, 200))\n",
            "                  st.image(image_resized,caption='Query Image') \n",
            "                  \n",
            "                  \n",
            "                  col1,col2,col3,col4,col5 = st.columns(5)\n",
            "                  with col1:\n",
            "                    st.write('유사상품1')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[0, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)\n",
            "                  with col2:\n",
            "                    st.write('유사상품2')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[1, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)   \n",
            "                  with col3:\n",
            "                    st.write('유사상품3')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[2, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)   \n",
            "                  with col4:\n",
            "                    st.write('유사상품4')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[3, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)   \n",
            "                  with col5:\n",
            "                    st.write('유사상품5')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[4, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)\n",
            "                  st.markdown('---') \n",
            "\n",
            "          \n",
            "              if set_1.isdisjoint(set_5):\n",
            "                    st.markdown('### 신발')\n",
            "                    st.write('신발을 찾지 못하였습니다.')\n",
            "                    st.markdown('---')\n",
            "              else:\n",
            "                for i in set_1.intersection(set_5):\n",
            "                  st.markdown('### 신발')\n",
            "                  Midclass = load_model(\"models/Midcateg_\" + i + \"_1216.h5\", compile=False)\n",
            "                  df = pd.read_csv(\"DB/df_styleReview_\" + i + \"_sim_matrix.csv\", dtype=object)\n",
            "              \n",
            "                  # 관련된 미리 학습된 siamse df을 불러와 준다. \n",
            "                  \n",
            "                  tqdm.pandas()\n",
            "                  xmin = df1.loc[df1['class_id'] == i,'xmin'].values[0]\n",
            "                  ymax = df1.loc[df1['class_id'] == i,'ymax'].values[0]\n",
            "                  xmax = df1.loc[df1['class_id'] == i,'xmax'].values[0]\n",
            "                  ymin = df1.loc[df1['class_id'] == i,'ymin'].values[0]\n",
            "                  img = cv2.imread(\"image/query.jpg\")\n",
            "                  crop_img = img[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
            "                  cv2.imwrite('image/croppedImage' + i + '.PNG',crop_img)\n",
            "                  query_dir = 'image/croppedImage' + i + '.PNG' \n",
            "                  query = query_dir\n",
            "                  # 여기에 크롭된 이미지를 넣어주어야 한다. -> 추후 수정예정  \n",
            "                  query_image = preprocess_image(query)\n",
            "                  # 크롭된 이미지를 넣어주어서 불러온다. \n",
            "                  query_embedding_ = Midclass(query_image)[0].numpy().astype(np.float32).tolist()\n",
            "                  query_embedding_ = str(query_embedding_)\n",
            "\n",
            "                  df['distance_samImg_queryRev'] = df['embedding_sample'].progress_apply(lambda x: np.linalg.norm(np.asarray(eval(x), dtype=np.float32) - np.asarray(eval(query_embedding_), dtype=np.float32)))\n",
            "                  df_sim_matrix = df.sort_values(by='distance_samImg_queryRev').reset_index(drop=True)\n",
            "                  # 이밑은 image로 해준다. print은 write로 바꿔준다.\n",
            "                  \n",
            "                  image_ = io.imread(query)\n",
            "                  image_resized = resize(image_, (200, 200))\n",
            "                  st.image(image_resized,caption='Query Image') \n",
            "                  \n",
            "                  col1,col2,col3,col4,col5 = st.columns(5)\n",
            "                  with col1:\n",
            "                    st.write('유사상품1')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[0, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)\n",
            "                  with col2:\n",
            "                    st.write('유사상품2')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[1, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)   \n",
            "                  with col3:\n",
            "                    st.write('유사상품3')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[2, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)   \n",
            "                  with col4:\n",
            "                    st.write('유사상품4')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[3, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)   \n",
            "                  with col5:\n",
            "                    st.write('유사상품5')\n",
            "                    row = df_sim_matrix.drop_duplicates(\"goods_id_pure\",inplace=False).iloc[4, :]\n",
            "                    st.image('image/' + load_image(row[\"goods_id_pure\"]), caption = '상품이미지')\n",
            "                    link = \"https://musinsa.com/app/goods/\" + row[\"goods_id_pure\"]\n",
            "                    product =  '[링크]' + '(' + link + ')'\n",
            "                    st.markdown(product, unsafe_allow_html=True)\n",
            "                  st.markdown('---')   \n",
            "      else: \n",
            "            pass\n",
            "    except:\n",
            "      st.write('상품을 인식하지 못하였습니다.')\n",
            "      st.markdown('---')       \n",
            "else:\n",
            "  st.write('이미지를 입력해 주세요.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat app_fake.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u_5gq929C1f",
        "outputId": "298f3efa-16ad-4b7c-91ef-63f610cac55f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import streamlit as st\n",
            "import numpy as np\n",
            "from detection.object_detection import detect_object\n",
            "import cv2\n",
            "import os\n",
            "from yolov7 import YOLOv7\n",
            "from PIL import Image, ImageOps\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from skimage import io\n",
            "from image_save import load_image\n",
            "\n",
            "st.set_page_config(page_title=\"AI Style Recommender\")\n",
            "\n",
            "st.image('logo.png')\n",
            "\n",
            "st.sidebar.title(\"AI Style Recommender\")\n",
            "st.sidebar.caption(\"Buy the style you want.\")\n",
            "st.sidebar.markdown(\"Made by Style Research Team\")\n",
            "st.sidebar.markdown(\"---\")\n",
            "\n",
            "# 여기서는 column의 형식으로 \n",
            "\n",
            "st.sidebar.header(\"Our DB\")\n",
            "st.sidebar.image(\"https://media.giphy.com/media/l8DnhngGazFTPhNLK6/giphy.gif\",width=300)\n",
            "st.sidebar.markdown('---')\n",
            "\n",
            "\n",
            "st.write('# Detect Your Style')\n",
            "\n",
            "uploaded_image = st.file_uploader(\"스타일 리뷰 이미지를 넣어주세요.\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
            "\n",
            "if uploaded_image is not None:\n",
            "    file_bytes = np.asarray(bytearray(uploaded_image.read()), dtype=np.uint8)\n",
            "    opencv_image = cv2.imdecode(file_bytes, 1)\n",
            "    st.image(opencv_image,caption='입력한 이미지' ,channels=\"BGR\")\n",
            "    image = detect_object(opencv_image)\n",
            "    cv2.imwrite(\"Detected_objects.jpg\",image)\n",
            "    st.image('Detected_objects.jpg',caption='스타일 인식')\n",
            "\n",
            "\n",
            "    st.markdown(\"## **Detection 결과**\")\n",
            " \n",
            "    # df = pd.read_csv()\n",
            "    \n",
            "    # st.dataframe(df, 800, 500)\n",
            "\n",
            "\n",
            "    st.markdown('---')\n",
            "     \n",
            "    goods_id_1 = [2173686, 2293954, 2039431, 1288281, 2282041]\n",
            "    goods_id_2 = [2107115,1826764,655627,1323090,2784138]\n",
            "    goods_id_3 = [2784034, 2633375,2801305,2633379,2820557]\n",
            "\n",
            "    st.write(\"## 어떤 상품일까?\")\n",
            "    col1,col2,col3 = st.columns(3)\n",
            "\n",
            "    with col1:\n",
            "        st.write(\"상의\")\n",
            "        j = 0\n",
            "        for i in goods_id_1:\n",
            "            j += 1\n",
            "            st.image(load_image(i), caption = '상품이미지')\n",
            "            link = \"https://musinsa.com/app/goods/\" + str(i)\n",
            "            product = \"상품\" + str(j) + \"링크  :\"+ (link)\n",
            "            st.write(product)\n",
            "\n",
            "    with col2:\n",
            "        st.write(\"바지\")\n",
            "        j = 0\n",
            "        for i in goods_id_2:\n",
            "            j += 1\n",
            "            st.image(load_image(i), caption = '상품이미지')\n",
            "            link = \"https://musinsa.com/app/goods/\" + str(i)\n",
            "            product = \"상품\" + str(j) + \" 링크  :\"+ (link)\n",
            "            st.write(product)\n",
            "\n",
            "    with col3:\n",
            "        st.write(\"신발\")\n",
            "        j = 0\n",
            "        for i in goods_id_3:\n",
            "            j += 1\n",
            "            st.image(load_image(i), caption = '상품이미지')\n",
            "            link = \"https://musinsa.com/app/goods/\" + str(i)\n",
            "            product = \"상품\" + str(j) + \" 링크  :\"+ (link)\n",
            "            st.write(product)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가짜 웹페이지 만들기\n",
        "from pyngrok import ngrok\n",
        "#!nohup streamlit run app.py\n",
        "!streamlit run app_fake.py&>/dev/null&\n",
        "publ_url = ngrok.connect(addr='8501')\n",
        "publ_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmkU11t_Vxa1",
        "outputId": "ceb8d642-0a0e-4b28-f129-32812a3691e6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.ngrok:Opening tunnel named: http-8501-f7d9ff8a-052a-4264-90bb-fed43224ad5f\n",
            "2022-12-19 11:12:56.778 INFO    pyngrok.ngrok: Opening tunnel named: http-8501-f7d9ff8a-052a-4264-90bb-fed43224ad5f\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T11:12:56+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "2022-12-19 11:12:56.836 INFO    pyngrok.process.ngrok: t=2022-12-19T11:12:56+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T11:12:56+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "2022-12-19 11:12:56.856 INFO    pyngrok.process.ngrok: t=2022-12-19T11:12:56+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T11:12:56+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "2022-12-19 11:12:56.870 INFO    pyngrok.process.ngrok: t=2022-12-19T11:12:56+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T11:12:56+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "2022-12-19 11:12:56.886 INFO    pyngrok.process.ngrok: t=2022-12-19T11:12:56+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T11:12:57+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "2022-12-19 11:12:57.507 INFO    pyngrok.process.ngrok: t=2022-12-19T11:12:57+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T11:12:57+0000 lvl=info msg=\"client session established\" obj=csess id=94ed31843017\n",
            "2022-12-19 11:12:57.514 INFO    pyngrok.process.ngrok: t=2022-12-19T11:12:57+0000 lvl=info msg=\"client session established\" obj=csess id=94ed31843017\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T11:12:57+0000 lvl=info msg=start pg=/api/tunnels id=103536bc4446b6df\n",
            "2022-12-19 11:12:57.560 INFO    pyngrok.process.ngrok: t=2022-12-19T11:12:57+0000 lvl=info msg=start pg=/api/tunnels id=103536bc4446b6df\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T11:12:57+0000 lvl=info msg=end pg=/api/tunnels id=103536bc4446b6df status=200 dur=707.734µs\n",
            "2022-12-19 11:12:57.584 INFO    pyngrok.process.ngrok: t=2022-12-19T11:12:57+0000 lvl=info msg=end pg=/api/tunnels id=103536bc4446b6df status=200 dur=707.734µs\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T11:12:57+0000 lvl=info msg=start pg=/api/tunnels id=49b0c8ed672cbbd1\n",
            "2022-12-19 11:12:57.619 INFO    pyngrok.process.ngrok: t=2022-12-19T11:12:57+0000 lvl=info msg=start pg=/api/tunnels id=49b0c8ed672cbbd1\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T11:12:57+0000 lvl=info msg=end pg=/api/tunnels id=49b0c8ed672cbbd1 status=200 dur=232.756µs\n",
            "2022-12-19 11:12:57.623 INFO    pyngrok.process.ngrok: t=2022-12-19T11:12:57+0000 lvl=info msg=end pg=/api/tunnels id=49b0c8ed672cbbd1 status=200 dur=232.756µs\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T11:12:57+0000 lvl=info msg=start pg=/api/tunnels id=afe25f55b1118052\n",
            "2022-12-19 11:12:57.627 INFO    pyngrok.process.ngrok: t=2022-12-19T11:12:57+0000 lvl=info msg=start pg=/api/tunnels id=afe25f55b1118052\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T11:12:58+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=\"http-8501-f7d9ff8a-052a-4264-90bb-fed43224ad5f (http)\" addr=http://localhost:8501 url=http://19b9-34-80-141-197.ngrok.io\n",
            "2022-12-19 11:12:58.011 INFO    pyngrok.process.ngrok: t=2022-12-19T11:12:58+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=\"http-8501-f7d9ff8a-052a-4264-90bb-fed43224ad5f (http)\" addr=http://localhost:8501 url=http://19b9-34-80-141-197.ngrok.io\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T11:12:58+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8501-f7d9ff8a-052a-4264-90bb-fed43224ad5f addr=http://localhost:8501 url=https://19b9-34-80-141-197.ngrok.io\n",
            "2022-12-19 11:12:58.022 INFO    pyngrok.process.ngrok: t=2022-12-19T11:12:58+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8501-f7d9ff8a-052a-4264-90bb-fed43224ad5f addr=http://localhost:8501 url=https://19b9-34-80-141-197.ngrok.io\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T11:12:58+0000 lvl=info msg=end pg=/api/tunnels id=afe25f55b1118052 status=201 dur=395.464946ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://19b9-34-80-141-197.ngrok.io\" -> \"http://localhost:8501\">"
            ]
          },
          "metadata": {},
          "execution_count": 42
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-12-19 11:12:58.036 INFO    pyngrok.process.ngrok: t=2022-12-19T11:12:58+0000 lvl=info msg=end pg=/api/tunnels id=afe25f55b1118052 status=201 dur=395.464946ms\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T11:12:58+0000 lvl=info msg=start pg=\"/api/tunnels/http-8501-f7d9ff8a-052a-4264-90bb-fed43224ad5f (http)\" id=87317da01a0b0896\n",
            "2022-12-19 11:12:58.048 INFO    pyngrok.process.ngrok: t=2022-12-19T11:12:58+0000 lvl=info msg=start pg=\"/api/tunnels/http-8501-f7d9ff8a-052a-4264-90bb-fed43224ad5f (http)\" id=87317da01a0b0896\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T11:12:58+0000 lvl=info msg=end pg=\"/api/tunnels/http-8501-f7d9ff8a-052a-4264-90bb-fed43224ad5f (http)\" id=87317da01a0b0896 status=200 dur=1.097956ms\n",
            "2022-12-19 11:12:58.057 INFO    pyngrok.process.ngrok: t=2022-12-19T11:12:58+0000 lvl=info msg=end pg=\"/api/tunnels/http-8501-f7d9ff8a-052a-4264-90bb-fed43224ad5f (http)\" id=87317da01a0b0896 status=200 dur=1.097956ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 실행하고 제대로 안 돌아갈때 \n",
        "ps하고 관련 코드들을 지워줘야 새로운 웹사이트 생성 가능 "
      ],
      "metadata": {
        "id": "lZGstJMxW9zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fA5Xh1vBXCdC",
        "outputId": "ee388d3d-ddd9-4bde-8d3a-f60b0c702799",
        "collapsed": true
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    PID TTY          TIME CMD\n",
            "      1 ?        00:00:02 docker-init\n",
            "      7 ?        00:00:41 node\n",
            "     18 ?        00:00:03 tail\n",
            "     32 ?        00:00:00 run.sh\n",
            "     34 ?        00:00:11 kernel_manager_\n",
            "     45 ?        00:00:05 python3 <defunct>\n",
            "     46 ?        00:00:22 colab-fileshim.\n",
            "     60 ?        00:00:30 jupyter-noteboo\n",
            "     61 ?        00:00:33 dap_multiplexer\n",
            "    241 ?        00:03:14 python3\n",
            "    256 ?        00:01:06 python3\n",
            "    328 ?        00:00:00 bash\n",
            "    329 ?        00:00:00 drive\n",
            "    330 ?        00:00:00 grep\n",
            "    404 ?        00:03:52 drive\n",
            "    435 ?        00:00:00 fusermount <defunct>\n",
            "    485 ?        00:00:00 bash\n",
            "    486 ?        00:00:03 directoryprefet\n",
            "    487 ?        00:00:02 tail\n",
            "    488 ?        00:00:00 python3\n",
            "   2822 ?        00:15:39 streamlit\n",
            "   2824 ?        00:00:52 ngrok\n",
            "   5250 ?        00:00:13 node\n",
            "   5275 ?        00:00:03 streamlit\n",
            "   5289 ?        00:00:00 ps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kill 5275"
      ],
      "metadata": {
        "id": "GI-GcWKkXEAI"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 진짜 웹페이지 만들기\n",
        "from pyngrok import ngrok\n",
        "#!nohup streamlit run app.py\n",
        "!streamlit run app.py&>/dev/null&\n",
        "publ_url = ngrok.connect(addr='8501')\n",
        "publ_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBSfa8guNzVi",
        "outputId": "33018780-f954-4790-e9ad-58597c590041"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.ngrok:Opening tunnel named: http-8501-2f68bd43-099f-4760-a8d9-d9c18ea68eda\n",
            "2022-12-19 20:01:26.136 INFO    pyngrok.ngrok: Opening tunnel named: http-8501-2f68bd43-099f-4760-a8d9-d9c18ea68eda\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T20:01:26+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "2022-12-19 20:01:26.202 INFO    pyngrok.process.ngrok: t=2022-12-19T20:01:26+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T20:01:26+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "2022-12-19 20:01:26.206 INFO    pyngrok.process.ngrok: t=2022-12-19T20:01:26+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T20:01:26+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "2022-12-19 20:01:26.218 INFO    pyngrok.process.ngrok: t=2022-12-19T20:01:26+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T20:01:26+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "2022-12-19 20:01:26.226 INFO    pyngrok.process.ngrok: t=2022-12-19T20:01:26+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T20:01:26+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "2022-12-19 20:01:26.849 INFO    pyngrok.process.ngrok: t=2022-12-19T20:01:26+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T20:01:26+0000 lvl=info msg=\"client session established\" obj=csess id=e9a5b41b710e\n",
            "2022-12-19 20:01:26.854 INFO    pyngrok.process.ngrok: t=2022-12-19T20:01:26+0000 lvl=info msg=\"client session established\" obj=csess id=e9a5b41b710e\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T20:01:26+0000 lvl=info msg=start pg=/api/tunnels id=a311d49d002293d2\n",
            "2022-12-19 20:01:26.878 INFO    pyngrok.process.ngrok: t=2022-12-19T20:01:26+0000 lvl=info msg=start pg=/api/tunnels id=a311d49d002293d2\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T20:01:26+0000 lvl=info msg=end pg=/api/tunnels id=a311d49d002293d2 status=200 dur=612.372µs\n",
            "2022-12-19 20:01:26.886 INFO    pyngrok.process.ngrok: t=2022-12-19T20:01:26+0000 lvl=info msg=end pg=/api/tunnels id=a311d49d002293d2 status=200 dur=612.372µs\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T20:01:26+0000 lvl=info msg=start pg=/api/tunnels id=d959a5cef5c5439f\n",
            "2022-12-19 20:01:26.893 INFO    pyngrok.process.ngrok: t=2022-12-19T20:01:26+0000 lvl=info msg=start pg=/api/tunnels id=d959a5cef5c5439f\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T20:01:26+0000 lvl=info msg=end pg=/api/tunnels id=d959a5cef5c5439f status=200 dur=155.841µs\n",
            "2022-12-19 20:01:26.899 INFO    pyngrok.process.ngrok: t=2022-12-19T20:01:26+0000 lvl=info msg=end pg=/api/tunnels id=d959a5cef5c5439f status=200 dur=155.841µs\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T20:01:26+0000 lvl=info msg=start pg=/api/tunnels id=a19f75d8a9d84286\n",
            "2022-12-19 20:01:26.902 INFO    pyngrok.process.ngrok: t=2022-12-19T20:01:26+0000 lvl=info msg=start pg=/api/tunnels id=a19f75d8a9d84286\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T20:01:27+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=\"http-8501-2f68bd43-099f-4760-a8d9-d9c18ea68eda (http)\" addr=http://localhost:8501 url=http://9003-34-80-141-197.ngrok.io\n",
            "2022-12-19 20:01:27.292 INFO    pyngrok.process.ngrok: t=2022-12-19T20:01:27+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=\"http-8501-2f68bd43-099f-4760-a8d9-d9c18ea68eda (http)\" addr=http://localhost:8501 url=http://9003-34-80-141-197.ngrok.io\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T20:01:27+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8501-2f68bd43-099f-4760-a8d9-d9c18ea68eda addr=http://localhost:8501 url=https://9003-34-80-141-197.ngrok.io\n",
            "2022-12-19 20:01:27.306 INFO    pyngrok.process.ngrok: t=2022-12-19T20:01:27+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8501-2f68bd43-099f-4760-a8d9-d9c18ea68eda addr=http://localhost:8501 url=https://9003-34-80-141-197.ngrok.io\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T20:01:27+0000 lvl=info msg=end pg=/api/tunnels id=a19f75d8a9d84286 status=201 dur=405.811268ms\n",
            "2022-12-19 20:01:27.316 INFO    pyngrok.process.ngrok: t=2022-12-19T20:01:27+0000 lvl=info msg=end pg=/api/tunnels id=a19f75d8a9d84286 status=201 dur=405.811268ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://9003-34-80-141-197.ngrok.io\" -> \"http://localhost:8501\">"
            ]
          },
          "metadata": {},
          "execution_count": 60
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process.ngrok:t=2022-12-19T20:01:27+0000 lvl=info msg=start pg=\"/api/tunnels/http-8501-2f68bd43-099f-4760-a8d9-d9c18ea68eda (http)\" id=bf4fb14d2448e7db\n",
            "2022-12-19 20:01:27.325 INFO    pyngrok.process.ngrok: t=2022-12-19T20:01:27+0000 lvl=info msg=start pg=\"/api/tunnels/http-8501-2f68bd43-099f-4760-a8d9-d9c18ea68eda (http)\" id=bf4fb14d2448e7db\n",
            "INFO:pyngrok.process.ngrok:t=2022-12-19T20:01:27+0000 lvl=info msg=end pg=\"/api/tunnels/http-8501-2f68bd43-099f-4760-a8d9-d9c18ea68eda (http)\" id=bf4fb14d2448e7db status=200 dur=164.761µs\n",
            "2022-12-19 20:01:27.338 INFO    pyngrok.process.ngrok: t=2022-12-19T20:01:27+0000 lvl=info msg=end pg=\"/api/tunnels/http-8501-2f68bd43-099f-4760-a8d9-d9c18ea68eda (http)\" id=bf4fb14d2448e7db status=200 dur=164.761µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KK14Ca4C-YRP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}